{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Discrminant Analysis\n",
    "\n",
    "Gaussian Discriminant Analysis is a simple Bayes Classifier wherein the Class Conditional Densities are Assumed to be Gaussian with some parameters. The parameters are learnt by Maximum Likelihood Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GDA():\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.classes = np.array(list(range(y_dim)))\n",
    "        self.initialize_parameters(x_dim, y_dim)\n",
    "        \n",
    "    def initialize_parameters(self, x_dim, y_dim):\n",
    "        meu = {}\n",
    "        sigma = {}\n",
    "        priors = {}\n",
    "        \n",
    "        for i in range(y_dim):\n",
    "            meu[i] = np.zeros((x_dim))\n",
    "            sigma[i] = np.zeros((x_dim, x_dim))\n",
    "        self.meu = meu\n",
    "        self.sigma = sigma\n",
    "        self.priors = priors\n",
    "    \n",
    "    def train(self, data, labels):\n",
    "        classes = set(list(labels))\n",
    "        \n",
    "        for c in classes:\n",
    "            class_data = data[labels == c]\n",
    "            self.learn_parameters(class_data, c)\n",
    "            self.priors[c] = float(len(class_data)) / len(data)\n",
    "            \n",
    "    def learn_parameters(self, class_data, c):\n",
    "        self.meu[c] = np.mean(class_data, axis = 0)\n",
    "        self.sigma[c] = np.dot((class_data - self.meu[c]).T, (class_data - self.meu[c]))\n",
    "        \n",
    "    def test(self, data, labels):\n",
    "        preds = np.zeros((data.shape[0]))\n",
    "        for i in range(data.shape[0]):\n",
    "            preds[i] = self.predict(data[i])\n",
    "        return preds, np.mean(preds == labels)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        probs = np.zeros((len(self.classes)))\n",
    "        for c in self.classes:\n",
    "            probs[c] = self.priors[c] * (1. / (np.power(2 * np.pi, self.x_dim / 2.) * np.power(np.linalg.det(self.sigma[c]), 0.5))) * np.exp(- 0.5 * np.dot((data - self.meu[c]).T, np.dot(np.linalg.inv(self.sigma[c]), (data - self.meu[c]))))\n",
    "        return np.argmax(probs / probs.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Toy Dataset IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\", header = None)\n",
    "\n",
    "X = df.loc[:, [0, 1, 2, 3]].values\n",
    "y = df.loc[:, 4].values\n",
    "\n",
    "cy = list(set(y))\n",
    "new_y = [0] * len(y)\n",
    "for i in range(len(y)):\n",
    "    new_y[i] = cy.index(y[i])\n",
    "new_y = np.array(new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GDA(X.shape[1], len(cy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, acc = model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.733333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Discriminant Analysis on MNIST Data, using PCA to reduce feature dimensions\n",
    "\n",
    "We would need to reduce data dimensions to a small number so that it becomes practical. We use PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, data):\n",
    "        self.num_data = data.shape[0]\n",
    "        self.dimension = data.shape[1]\n",
    "        self.data = data\n",
    "        self.covariance = self.compute_covariance(data)\n",
    "        self.principal_components, self.principal_axes = self.compute_pca()\n",
    "    \n",
    "    def compute_covariance(self, X):\n",
    "        return np.cov(X.T)\n",
    "    \n",
    "    def compute_pca(self):\n",
    "        eigen_values, eigen_vectors = np.linalg.eig(self.covariance)\n",
    "        eigen_values, eigen_vectors = eigen_values.real, eigen_vectors.real\n",
    "        eigen = []\n",
    "        for i in range(len(eigen_values)):\n",
    "            eigen.append((eigen_values[i], eigen_vectors[:, i]))\n",
    "        eigen.sort(key=lambda x: x[0], reverse = True)\n",
    "        principal_axes = np.array([x[1] for x in eigen])\n",
    "        principal_comps = np.array([x[0] for x in eigen])\n",
    "        return principal_comps, principal_axes\n",
    "    \n",
    "    def get_principal_axes(self, num):\n",
    "        return self.principal_axes[:num]\n",
    "    \n",
    "    def get_projection(self, num, X):\n",
    "        n_axes = self.get_principal_axes(num)\n",
    "        projections = np.dot(n_axes, X.T)\n",
    "        return projections.T\n",
    "    \n",
    "    def inverse_PCA(self, X):\n",
    "        num = X.shape[1]\n",
    "        n_axes = self.get_principal_axes(num)\n",
    "        original_projections = np.dot(X, n_axes)\n",
    "        return original_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the MNIST Dataset\n",
    "##### We only work with 1000 training and 200 test data to keep the model fast and simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnist.csv\", header=0, index_col=0)\n",
    "data_flat = {}\n",
    "data_image = {}\n",
    "for i in range(9):\n",
    "    x = df[df.index == i]\n",
    "    data_flat[i] = np.array(x)\n",
    "    x = np.array(x).reshape(len(x), 28, 28)\n",
    "    data_image[i] = x\n",
    "    \n",
    "X_train = np.array(df)[:1000]\n",
    "X_test = np.array(df)[1000:1200]\n",
    "y_train = np.array(df.index)[:1000].astype(np.int8).reshape(-1)\n",
    "y_test = np.array(df.index)[1000:1200].astype(np.int8).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA is trained/learnt using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first 50 principal components are extracted from both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2_train = pca.get_projection(50, X_train)\n",
    "X2_test = pca.get_projection(50, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GDA(X2_train.shape[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.train(X2_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krishanu/anaconda2/lib/python2.7/site-packages/numpy/linalg/linalg.py:1821: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    }
   ],
   "source": [
    "preds, acc = model.test(X2_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.29\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of the GDA model on the MNIST Dataset is only 29%. This is better than random chance (10%), but much lower than other more powerful bayes classifier models which model the class conditional densities as more powerful distributions like GMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naiive Bayes Model\n",
    "\n",
    "In this part we will use a Naiive Bayes Model for the tasks.\n",
    "Each of the constituent probability distributions are modelled using a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NB():\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.classes = np.array(list(range(y_dim)))\n",
    "        self.initialize_parameters(x_dim, y_dim)\n",
    "        \n",
    "    def initialize_parameters(self, x_dim, y_dim):\n",
    "        mu = np.zeros((y_dim, x_dim))\n",
    "        sigma = np.zeros((y_dim, x_dim))\n",
    "        priors = np.zeros((y_dim))\n",
    "            \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.priors = priors\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        classes = set(list(labels))\n",
    "        \n",
    "        for c in classes:\n",
    "            class_data = data[labels == c]\n",
    "            self.learn_parameters(class_data, c)\n",
    "            self.priors[c] = float(len(class_data)) / len(data)\n",
    "            \n",
    "    def learn_parameters(self, class_data, c):\n",
    "        self.mu[c] = np.mean(class_data, axis = 0)\n",
    "        self.sigma[c] = np.var(class_data, axis = 0)\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        preds = np.zeros((data.shape[0]))\n",
    "        for i in range(data.shape[0]):\n",
    "            preds[i] = self.predict(data[i])\n",
    "        return preds, np.mean(preds == labels)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        logprobs = np.log(self.priors)\n",
    "        for c in self.classes:\n",
    "            for i in range(self.x_dim):\n",
    "                logprobs[c] += np.log(self.gaussian_prob(data[i], i, c))\n",
    "        return np.argmax(logprobs)\n",
    "    \n",
    "    def gaussian_prob(self, x, i, c):\n",
    "        val = 1. / (np.power(2 * np.pi * self.sigma[c, i], 0.5)) * np.exp(- 0.5 * np.power(x - self.mu[c, i], 2) / self.sigma[c, i])\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB for iris type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.866666666667\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"iris.csv\", header = None)\n",
    "\n",
    "X = df.loc[:, [0, 1, 2, 3]].values\n",
    "y = df.loc[:, 4].values\n",
    "\n",
    "cy = list(set(y))\n",
    "new_y = [0] * len(y)\n",
    "for i in range(len(y)):\n",
    "    new_y[i] = cy.index(y[i])\n",
    "new_y = np.array(new_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.10)\n",
    "model = NB(X.shape[1], len(cy))\n",
    "\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "preds, acc = model.test(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy of the model: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Naiive bayes Model showed good accuracy. Now lets try this on the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB for MNIST Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.815\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mnist.csv\", header=0, index_col=0)\n",
    "data_flat = {}\n",
    "data_image = {}\n",
    "for i in range(9):\n",
    "    x = df[df.index == i]\n",
    "    data_flat[i] = np.array(x)\n",
    "    x = np.array(x).reshape(len(x), 28, 28)\n",
    "    data_image[i] = x\n",
    "    \n",
    "X_train = np.array(df)[:1000]\n",
    "X_test = np.array(df)[1000:1200]\n",
    "y_train = np.array(df.index)[:1000].astype(np.int8).reshape(-1)\n",
    "y_test = np.array(df.index)[1000:1200].astype(np.int8).reshape(-1)\n",
    "\n",
    "pca = PCA(X_train)\n",
    "X2_train = pca.get_projection(50, X_train)\n",
    "X2_test = pca.get_projection(50, X_test)\n",
    "\n",
    "model = NB(50, 10)\n",
    "\n",
    "model.train(X2_train, y_train)\n",
    "\n",
    "preds, acc = model.test(X2_test, y_test)\n",
    "\n",
    "print(\"Accuracy of the model: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy is quite high considering that they are very simple models, and that the amount of data used is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
